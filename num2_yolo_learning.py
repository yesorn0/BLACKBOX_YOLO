import sys
import os
import gc
import torch
import time
from pathlib import Path
from importlib.util import find_spec


def check_dependencies():
    """Check if required packages are installed and importable."""
    pkgs = {
        "numpy": "numpy",
        "torch": "torch",
        "ultralytics": "ultralytics",
        "cv2": "opencv-python",
        "yaml": "pyyaml"
    }
    missing = []
    for name, pip_name in pkgs.items():
        if find_spec(name) is None:
            missing.append(pip_name)
            print(f"\033[91mâœ— {name} ë¯¸ì„¤ì¹˜\033[0m")
        else:
            print(f"\033[92mâœ“ {name} ì„¤ì¹˜ë¨\033[0m")
    if missing:
        print(f"\nì•„ë˜ ëª…ë ¹ìœ¼ë¡œ ë¶€ì¡±í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:")
        print(f"pip install {' '.join(missing)}")
        return False
    return True


def optimize_system_for_6gb():
    """RTX 3060 6GB ì „ìš© ì‹œìŠ¤í…œ ìµœì í™”"""
    print("RTX 3060 6GB í™˜ê²½ì— ë§ì¶˜ ìµœì í™” ì ìš©...")

    # GPU ë©”ëª¨ë¦¬ ì ê·¹ì  ê´€ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

        # 6GB GPUë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64,garbage_collection_threshold:0.6'

        # cuDNN ìµœì í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ìš°ì„ )
        torch.backends.cudnn.benchmark = False  # ë©”ëª¨ë¦¬ ì ˆì•½
        torch.backends.cudnn.deterministic = True

        # ë©”ëª¨ë¦¬ í”„ë˜ê·¸ë©˜í…Œì´ì…˜ ë°©ì§€
        torch.cuda.set_per_process_memory_fraction(0.85)  # 6GBì˜ 85%ë§Œ ì‚¬ìš©

    # ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™” (Windows PyCharm í™˜ê²½)
    if sys.platform.startswith('win'):
        os.environ['OMP_NUM_THREADS'] = '2'  # 6GB GPUì— ë§ì¶˜ ì¡°ì •
        torch.set_num_threads(2)

    # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
    gc.set_threshold(700, 10, 10)  # ë” ìì£¼ ë©”ëª¨ë¦¬ ì •ë¦¬


def print_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
    import torch, numpy
    print(f"\n=== ì‹œìŠ¤í…œ ì •ë³´ ===")
    print(f"Python: {sys.version.split()[0]}")
    print(f"NumPy: {numpy.__version__}")
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDA: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
        print(f"GPU: {gpu_name}")
        print(f"GPU ì´ ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")

        # í˜„ì¬ GPU ì‚¬ìš©ëŸ‰
        allocated = torch.cuda.memory_allocated(0) / 1024 ** 3
        reserved = torch.cuda.memory_reserved(0) / 1024 ** 3
        print(f"GPU í• ë‹¹ë¨: {allocated:.2f}GB / ì˜ˆì•½ë¨: {reserved:.2f}GB")

        # 6GB GPU ê²½ê³ 
        if gpu_memory <= 6.5:
            print(f"âš ï¸  6GB GPU ê°ì§€ - ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë“œ í™œì„±í™”")
    else:
        print("âŒ CUDA ë¯¸ì‚¬ìš© - CPU ëª¨ë“œ (ë§¤ìš° ëŠë¦¼)")


def get_rtx3060_6gb_params():
    """RTX 3060 6GB ì „ìš© ìµœì í™” íŒŒë¼ë¯¸í„°"""
    if not torch.cuda.is_available():
        return {
            'epochs': 50,
            'imgsz': 320,  # CPUëŠ” ë§¤ìš° ì‘ì€ ì´ë¯¸ì§€
            'batch': 2,
            'workers': 1,
            'amp': False,
            'cache': False
        }

    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3

    # RTX 3060 6GB ì „ìš© ì„¤ì •
    if gpu_memory <= 6.5:  # 6GB GPU
        params = {
            # ê¸°ë³¸ í›ˆë ¨ ì„¤ì •
            'epochs': 50,  # ë” ë§ì€ ì—í¬í¬ë¡œ ì„±ëŠ¥ ë³´ì™„
            'imgsz': 384,  # 6GBì— ìµœì í™”ëœ ì´ë¯¸ì§€ í¬ê¸°
            'batch': 8,  # 6GBì— ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
            'workers': 6,  # PyCharm ê°€ìƒí™˜ê²½ì— ì í•©
            'amp': True,  # í˜¼í•© ì •ë°€ë„ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            'cache': False,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìºì‹œ ë¹„í™œì„±í™”
            'device': 0,
            'patience': 30,  # ë” ê¸´ patience
            'save_period': 3,  # 3 ì—í¬í¬ë§ˆë‹¤ ì €ì¥

            # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (6GB ìµœì í™”)
            'optimizer': 'AdamW',
            'lr0': 0.0008,  # ì•½ê°„ ë‚®ì€ í•™ìŠµë¥ 
            'lrf': 0.05,  # ë” ê°•í•œ ìŠ¤ì¼€ì¤„ë§
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 5,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,

            # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜ (6GB í™˜ê²½ ìµœì í™”)
            'box': 7.5,
            'cls': 0.3,  # ë¶„ë¥˜ ì†ì‹¤ ì•½ê°„ ê°ì†Œ
            'dfl': 1.5,
            'pose': 12.0,
            'kobj': 1.0,  # ê°ì²´ì„± ì†ì‹¤ ê°ì†Œ

            # ë°ì´í„° ì¦ê°• (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            'hsv_h': 0.01,  # ìƒ‰ìƒ ì¦ê°• ê°ì†Œ
            'hsv_s': 0.5,
            'hsv_v': 0.3,
            'degrees': 0.0,
            'translate': 0.05,  # ì´ë™ ì¦ê°• ê°ì†Œ
            'scale': 0.8,  # ìŠ¤ì¼€ì¼ ì¦ê°• ê°ì†Œ
            'shear': 0.0,
            'perspective': 0.0,
            'flipud': 0.0,
            'fliplr': 0.5,
            'mosaic': 0.8,  # ëª¨ìì´í¬ í™•ë¥  ê°ì†Œ
            'mixup': 0.0,  # mixup ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            'copy_paste': 0.0  # copy_paste ë¹„í™œì„±í™”
        }
    else:
        # ë” í° GPUìš© ì„¤ì •
        params = {
            'epochs': 50,
            'imgsz': 384,
            'batch': 8,
            'workers': 8,
            'amp': True,
            'cache': 'ram',
            'device': 0,
            'patience': 20,
            'save_period': 3
        }

    return params


def validate_dataset(data_path):
    """ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬"""
    if not data_path.exists():
        print(f"\033[91mâŒ {data_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\033[0m")
        return False

    try:
        import yaml
        with open(data_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)

        required_keys = ['train', 'val', 'nc', 'names']
        for key in required_keys:
            if key not in data_config:
                print(f"\033[91mâŒ data.yamlì— '{key}' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤!\033[0m")
                return False

        print(f"\033[92mâœ“ ë°ì´í„°ì…‹ ì„¤ì • í™•ì¸ ì™„ë£Œ\033[0m")
        print(f"  - í´ë˜ìŠ¤ ìˆ˜: {data_config['nc']}")
        print(f"  - í´ë˜ìŠ¤ëª…: {data_config['names']}")

        # ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
        train_path = Path(data_config['train'])
        val_path = Path(data_config['val'])

        if not train_path.exists():
            print(f"\033[93mâš ï¸  í›ˆë ¨ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}\033[0m")
        if not val_path.exists():
            print(f"\033[93mâš ï¸  ê²€ì¦ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_path}\033[0m")

        return True

    except Exception as e:
        print(f"\033[91mâŒ data.yaml ì½ê¸° ì‹¤íŒ¨: {e}\033[0m")
        return False


def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()


def monitor_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024 ** 3
        reserved = torch.cuda.memory_reserved(0) / 1024 ** 3
        total = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
        print(f"GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB ì‚¬ìš© / {reserved:.2f}GB ì˜ˆì•½ / {total:.1f}GB ì´ëŸ‰")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìœ¼ë©´ ê²½ê³ 
        if reserved / total > 0.9:
            print("âš ï¸  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")


def cleanup_runs_folder():
    """ì´ì „ ì‹¤í–‰ ê²°ê³¼ ì •ë¦¬"""
    runs_path = Path('runs')
    if runs_path.exists():
        import shutil
        existing_runs = list(runs_path.glob('train/*'))
        if len(existing_runs) > 3:
            print(f"ì´ì „ í•™ìŠµ ê²°ê³¼ê°€ {len(existing_runs)}ê°œ ìˆìŠµë‹ˆë‹¤.")
            print("ì •ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ", end='')
            try:
                if input().lower() == 'y':
                    backup_name = f'runs_backup_{int(time.time())}'
                    shutil.move(str(runs_path), backup_name)
                    runs_path.mkdir()
                    print(f"ë°±ì—… ì™„ë£Œ: {backup_name}")
            except Exception as e:
                print(f"í´ë” ì •ë¦¬ ì‹¤íŒ¨: {e}")


def main():
    print("=" * 60)
    print("ğŸš€ RTX 3060 6GB ìµœì í™” YOLO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ v3.0")
    print("ğŸ’¡ PyCharm ê°€ìƒí™˜ê²½ + 6GB GPU ì „ìš© ìµœì í™”")
    print("=" * 60)

    # 1. íŒ¨í‚¤ì§€ í™•ì¸
    print("\n1ï¸âƒ£ í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸...")
    if not check_dependencies():
        print("\níŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        print("PyCharm Terminalì—ì„œ: pip install ultralytics opencv-python pyyaml")
        sys.exit(1)

    # 2. ì‹œìŠ¤í…œ ìµœì í™”
    print("\n2ï¸âƒ£ RTX 3060 6GB ìµœì í™” ì ìš©...")
    optimize_system_for_6gb()
    print_system_info()

    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    monitor_memory_usage()

    # 3. ë°ì´í„°ì…‹ í™•ì¸
    print("\n3ï¸âƒ£ ë°ì´í„°ì…‹ í™•ì¸...")
    data_path = Path('data.yaml')
    if not validate_dataset(data_path):
        print("\në°ì´í„°ì…‹ ì„¤ì •ì„ í™•ì¸í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # 4. ì´ì „ ê²°ê³¼ ì •ë¦¬
    print("\n4ï¸âƒ£ ì´ì „ í•™ìŠµ ê²°ê³¼ í™•ì¸...")
    cleanup_runs_folder()

    try:
        from ultralytics import YOLO

        print("\n5ï¸âƒ£ YOLO ëª¨ë¸ ë¡œë”©...")
        # RTX 3060 6GBì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ
        print("RTX 3060 6GBì— ìµœì í™”ëœ YOLOv8s ëª¨ë¸ ì‚¬ìš©")
        model = YOLO('yolov8m.pt')  # Small ëª¨ë¸ì´ 6GBì— ìµœì 

        print("\n6ï¸âƒ£ RTX 3060 6GB ìµœì í™” íŒŒë¼ë¯¸í„° ì„¤ì •...")
        params = get_rtx3060_6gb_params()

        print(f"\nğŸ“Š RTX 3060 6GB ìµœì í™” í•™ìŠµ ì„¤ì •:")
        for key, value in params.items():
            print(f"  - {key}: {value}")

        # ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ë” ì •í™•í•œ ì¶”ì •)
        estimated_time_per_epoch = 8566 / params['batch'] * 0.8  # 6GB GPU ë³´ì •
        total_estimated_time = estimated_time_per_epoch * params['epochs'] / 60
        print(f"\nâ±ï¸  ì˜ˆìƒ í•™ìŠµ ì‹œê°„: {total_estimated_time:.0f}ë¶„ ({total_estimated_time / 60:.1f}ì‹œê°„)")
        print("ğŸ“Œ 6GB GPU í™˜ê²½ì—ì„œëŠ” ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì‹œì‘
        cleanup_gpu_memory()

        print("\nğŸ YOLO í•™ìŠµ ì‹œì‘!")
        print("ğŸ’¡ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ê°€ ì¡°ì •ë©ë‹ˆë‹¤.")
        print("-" * 50)

        start_time = time.time()

        # í•™ìŠµ ì‹¤í–‰ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
        try:
            results = model.train(
                data='data.yaml',
                project='runs/train',
                name='optimized_exp',
                exist_ok=True,
                resume=False,
                verbose=True,
                **params
            )
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ìë™ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤...")
                params['batch'] = max(4, params['batch'] // 2)
                params['workers'] = max(2, params['workers'] // 2)
                print(f"ìƒˆë¡œìš´ ë°°ì¹˜ í¬ê¸°: {params['batch']}")

                cleanup_gpu_memory()

                # ì¬ì‹œë„
                results = model.train(
                    data='data.yaml',
                    exist_ok=True,
                    resume='runs/train/optimized_exp',
                    verbose=True,
                    **params
                )
            else:
                raise e

        end_time = time.time()
        training_time = end_time - start_time

        print("\n" + "=" * 60)
        print("ğŸ‰ RTX 3060 6GB ìµœì í™” í•™ìŠµ ì™„ë£Œ!")
        print(f"â° ì´ í•™ìŠµ ì‹œê°„: {training_time / 3600:.1f}ì‹œê°„")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results.save_dir}")
        print(f"ğŸ† ìµœê³  ëª¨ë¸: {results.save_dir}/weights/best.pt")
        print(f"ğŸ“Š ë§ˆì§€ë§‰ ëª¨ë¸: {results.save_dir}/weights/last.pt")
        print("\nğŸ’¡ PyCharmì—ì„œ ê²°ê³¼ í™•ì¸:")
        print(f"   - TensorBoard: tensorboard --logdir {results.save_dir}")
        print(f"   - ê²°ê³¼ ì´ë¯¸ì§€: {results.save_dir}/")
        print("=" * 60)

        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
        monitor_memory_usage()

    except KeyboardInterrupt:
        print("\n\nâŒ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ resume=True ì˜µì…˜ìœ¼ë¡œ ì´ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"\nâŒ RTX 3060 6GB ë©”ëª¨ë¦¬ ë¶€ì¡±!")
            print("ğŸ”§ í•´ê²° ë°©ë²•:")
            print("1. ë°°ì¹˜ í¬ê¸° ë” ì¤„ì´ê¸°: batch=8 ë˜ëŠ” batch=4")
            print("2. ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°: imgsz=320 ë˜ëŠ” imgsz=384")
            print("3. ì›Œì»¤ ìˆ˜ ì¤„ì´ê¸°: workers=2")
            print("4. í˜¼í•© ì •ë°€ë„ í™•ì¸: amp=True")
            print("5. PyCharmì—ì„œ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        else:
            print(f"\nâŒ RuntimeError: {e}")

    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        print("ğŸ” PyCharm ì½˜ì†”ì—ì„œ ì „ì²´ ì—ëŸ¬ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    finally:
        # í•­ìƒ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_gpu_memory()
        print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()