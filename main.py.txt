#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IoU ê¸°ë°˜ ì°¨ëŸ‰ ì¶”ì  ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ v8.1 + TTS ìµœì í™”
v4l2 ìë™ ì‹¤í–‰ + í•œêµ­ ì‹ í˜¸ë“± ìµœì í™” + ì°¨ì„  ê¸°ë°˜ ë™ì  ì•ì°¨ ê°ì§€
"""

import cv2
import numpy as np
import time
import json
import logging
import os
import signal
import sys
import psutil
from datetime import datetime
from collections import deque
import argparse
import queue
import threading
import hashlib
from tts_config import LOGGING_LEVEL, LOGGING_FORMAT
from tts_settings import TTSNavigationSystem

logging.basicConfig(level=getattr(logging, LOGGING_LEVEL), format=LOGGING_FORMAT)


class CameraManager:
    def __init__(self, logger):
        self.logger = logger
        self.camera = None
        
    def init_camera_auto(self):
        self.logger.info("ğŸ” v4l2 ì¹´ë©”ë¼ ë””ë°”ì´ìŠ¤ ìë™ íƒì§€ ì¤‘...")
        cap = cv2.VideoCapture(-1, cv2.CAP_V4L2)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret and frame is not None and frame.size > 0:
                self.camera = cap
                self.apply_camera_settings()
                self.logger.info("âœ… ì¹´ë©”ë¼ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
        self.logger.error("âŒ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨")
        cap.release()
        return False
    
    def apply_camera_settings(self):
        try:
            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            self.camera.set(cv2.CAP_PROP_FPS, 30)
            self.camera.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        except Exception as e:
            self.logger.warning(f"ì¹´ë©”ë¼ ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}")

class LaneDetector:
    """ì°¨ì„  ê°ì§€ ë° ë™ì  Front Zone ê³„ì‚°"""
    
    def __init__(self):
        self.lane_history = deque(maxlen=5)
        self.last_valid_lanes = None
        
    def detect_lanes(self, frame):
        height, width = frame.shape[:2]
        roi_vertices = np.array([
            [(0, height), (width//2 - 50, height*0.65),
             (width//2 + 50, height*0.65), (width, height)]
        ], dtype=np.int32)
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        edges = cv2.Canny(blur, 50, 150)
        
        mask = np.zeros_like(edges)
        cv2.fillPoly(mask, [roi_vertices], 255)
        masked_edges = cv2.bitwise_and(edges, mask)
        
        lines = cv2.HoughLinesP(
            masked_edges, 
            rho=1, 
            theta=np.pi/180, 
            threshold=50,
            minLineLength=50, 
            maxLineGap=50
        )
        
        return self.process_lane_lines(lines, width, height)
    
    def process_lane_lines(self, lines, width, height):
        if lines is None:
            return self.last_valid_lanes
        
        left_lines = []
        right_lines = []
        
        for line in lines:
            x1, y1, x2, y2 = line[0]
            if x2 - x1 == 0:
                continue
            slope = (y2 - y1) / (x2 - x1)
            
            if slope < -0.5:
                left_lines.append(line[0])
            elif slope > 0.5:
                right_lines.append(line[0])
        
        left_lane = self.average_lane(left_lines, width, height)
        right_lane = self.average_lane(right_lines, width, height)
        
        if left_lane is not None or right_lane is not None:
            lanes = {'left': left_lane, 'right': right_lane}
            self.lane_history.append(lanes)
            self.last_valid_lanes = lanes
            return lanes
        
        return self.last_valid_lanes
    
    def average_lane(self, lane_lines, width, height):
        if not lane_lines:
            return None
        
        x_coords = []
        y_coords = []
        for line in lane_lines:
            x1, y1, x2, y2 = line
            x_coords.extend([x1, x2])
            y_coords.extend([y1, y2])
        
        if len(x_coords) < 2:
            return None
        
        poly = np.polyfit(y_coords, x_coords, 1)
        y1 = height
        y2 = int(height * 0.6)
        x1 = int(poly[0] * y1 + poly[1])
        x2 = int(poly[0] * y2 + poly[1])
        
        return [x1, y1, x2, y2]
    
    def calculate_lane_center_points(self, lanes, height):
        if not lanes or (lanes['left'] is None and lanes['right'] is None):
            return None
        
        center_points = []
        for y in range(int(height * 0.65), height, 20):
            left_x = None
            right_x = None
            
            if lanes['left']:
                x1, y1, x2, y2 = lanes['left']
                if y2 != y1:
                    left_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            
            if lanes['right']:
                x1, y1, x2, y2 = lanes['right']
                if y2 != y1:
                    right_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            
            if left_x is not None and right_x is not None:
                center_x = (left_x + right_x) / 2
            elif left_x is not None:
                center_x = left_x + 60
            elif right_x is not None:
                center_x = right_x - 60
            else:
                continue
            
            center_points.append((int(center_x), y))
        
        return center_points

class SimpleIOUTracker:
    """IoU ê¸°ë°˜ ë‹¨ìˆœ ê°ì²´ ì¶”ì ê¸° (ì°¨ì„  ì¡´ ë‚´ ê°ë„ í–¥ìƒ)"""
    
    def __init__(self, max_lost=5, iou_threshold=0.3, movement_threshold=2):  # ì´ë™ ì„ê³„ê°’ 2ë¡œ ë‚®ì¶¤
        self.tracks = {}
        self.next_id = 1
        self.max_lost = max_lost
        self.iou_threshold = iou_threshold
        self.movement_threshold = movement_threshold
        self.frame_count = 0
        
    def calculate_iou(self, box1, box2):
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def detect_departure(self, track, is_in_zone=False):
        """ì¶œë°œ ê°ì§€: ì¡´ ë‚´ ì°¨ëŸ‰ì€ ë” ë¯¼ê°í•˜ê²Œ ê°ì§€"""
        if len(track['movement_history']) < 1:
            return False
            
        # ìµœê·¼ 1í”„ë ˆì„ ì´ë™ëŸ‰ë§Œìœ¼ë¡œë„ ê°ì§€
        move_x, move_y = track['movement_history'][-1]
        distance = (move_x**2 + move_y**2)**0.5
        
        # ì¡´ ë‚´ ì°¨ëŸ‰ì€ ì„ê³„ê°’ 50% ë‚®ì¶¤
        threshold = self.movement_threshold * 0.5 if is_in_zone else self.movement_threshold
        return distance > threshold
    
    def update(self, detections):
        self.frame_count += 1
        active_tracks = {tid: track for tid, track in self.tracks.items() if track['lost'] <= self.max_lost}
        matched_tracks = set()
        matched_detections = set()
        
        for track_id, track in active_tracks.items():
            best_iou = 0
            best_detection_idx = -1
            for i, detection in enumerate(detections):
                if i in matched_detections:
                    continue
                iou = self.calculate_iou(track['bbox'], detection['box'])
                if iou > best_iou and iou > self.iou_threshold:
                    best_iou = iou
                    best_detection_idx = i
            
            if best_detection_idx != -1:
                old_bbox = track['bbox']
                new_bbox = detections[best_detection_idx]['box']
                old_center_x = old_bbox[0] + old_bbox[2] / 2
                old_center_y = old_bbox[1] + old_bbox[3] / 2
                new_center_x = new_bbox[0] + new_bbox[2] / 2
                new_center_y = new_bbox[1] + new_bbox[3] / 2
                movement_x = new_center_x - old_center_x
                movement_y = new_center_y - old_center_y
                track['bbox'] = new_bbox
                track['confidence'] = detections[best_detection_idx]['confidence']
                track['lost'] = 0
                track['movement_history'].append((movement_x, movement_y))
                track['last_update'] = self.frame_count
                
                # ì¡´ ë‚´ ì°¨ëŸ‰ ì—¬ë¶€ í™•ì¸
                is_in_zone = detections[best_detection_idx].get('in_zone', False)
                if self.detect_departure(track, is_in_zone):
                    track['is_moving'] = True
                    track['departure_detected'] = True
                
                matched_tracks.add(track_id)
                matched_detections.add(best_detection_idx)
        
        for track_id, track in active_tracks.items():
            if track_id not in matched_tracks:
                track['lost'] += 1
        
        for i, detection in enumerate(detections):
            if i not in matched_detections:
                self.tracks[self.next_id] = {
                    'id': self.next_id,
                    'bbox': detection['box'],
                    'confidence': detection['confidence'],
                    'class_name': detection['class_name'],
                    'lost': 0,
                    'created_frame': self.frame_count,
                    'last_update': self.frame_count,
                    'movement_history': deque(maxlen=10),
                    'is_moving': False,
                    'departure_detected': False
                }
                self.next_id += 1
        
        self.tracks = {tid: track for tid, track in self.tracks.items() if track['lost'] <= self.max_lost}
        return self.get_active_tracks()
    
    def get_active_tracks(self):
        return [track for track in self.tracks.values() if track['lost'] == 0]

class TrafficLightColorDetector:
    """ì‹ í˜¸ë“± ìƒ‰ìƒ ê°ì§€ê¸° (ìƒ‰ìƒ ë²”ìœ„ ì •ë°€ ì¡°ì •)"""
    
    def __init__(self, stability_frames=2):
        self.stability_frames = stability_frames
        self.color_history = deque(maxlen=stability_frames)
        self.last_stable_color = None
        self.last_change_time = 0
        self.previous_color = None
        self.confidence_threshold = 0.5
    
    def detect_traffic_light_color(self, roi):
        if roi.size == 0:
            return None
        
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        
        # ìƒ‰ìƒ ë²”ìœ„ ì¬ì¡°ì • (í•œêµ­ ì‹ í˜¸ë“± ìµœì í™”)
        red_lower1 = np.array([0, 100, 100])    # ë°ê¸° ì¦ê°€
        red_upper1 = np.array([10, 255, 255])
        red_lower2 = np.array([170, 100, 100])   # ë°ê¸° ì¦ê°€
        red_upper2 = np.array([180, 255, 255])
        red_lower3 = np.array([0, 80, 80])      # ì±„ë„/ë°ê¸° í•˜í–¥
        red_upper3 = np.array([15, 255, 255])   # Hue ë²”ìœ„ í™•ì¥
        red_lower4 = np.array([165, 80, 80])    # ì±„ë„/ë°ê¸° í•˜í–¥
        red_upper4 = np.array([180, 255, 255])
        
        yellow_lower = np.array([18, 120, 120])  # ë²”ìœ„ ì¶•ì†Œ
        yellow_upper = np.array([35, 255, 255])
        
        green_lower = np.array([45, 100, 100])   # ë²”ìœ„ ì¶•ì†Œ
        green_upper = np.array([90, 255, 255])
        
        red_mask1 = cv2.inRange(hsv, red_lower1, red_upper1)
        red_mask2 = cv2.inRange(hsv, red_lower2, red_upper2)
        red_mask3 = cv2.inRange(hsv, red_lower3, red_upper3)
        red_mask4 = cv2.inRange(hsv, red_lower4, red_upper4)
        red_mask = red_mask1 + red_mask2 + red_mask3 + red_mask4
        
        yellow_mask = cv2.inRange(hsv, yellow_lower, yellow_upper)
        green_mask = cv2.inRange(hsv, green_lower, green_upper)
        
        red_pixels = cv2.countNonZero(red_mask)
        yellow_pixels = cv2.countNonZero(yellow_mask)
        green_pixels = cv2.countNonZero(green_mask)
        
        max_pixels = max(red_pixels, yellow_pixels, green_pixels)
        
        if max_pixels < 15:  # ì„ê³„ê°’ ìƒí–¥
            return None
        
        if red_pixels == max_pixels:
            return 'red'
        elif yellow_pixels == max_pixels:
            return 'yellow'
        elif green_pixels == max_pixels:
            return 'green'
        
        return None
    
    def update_color_history(self, color):
        if color:
            self.color_history.append(color)
        
        if len(self.color_history) >= self.stability_frames:
            color_counts = {}
            for c in self.color_history:
                color_counts[c] = color_counts.get(c, 0) + 1
            
            most_common_color = max(color_counts, key=color_counts.get)
            confidence = color_counts[most_common_color] / self.stability_frames
            
            if confidence >= self.confidence_threshold:
                if self.last_stable_color != most_common_color:
                    change_info = {
                        'datetime': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'timestamp': time.time()
                    }
                    self.previous_color = self.last_stable_color
                    self.last_stable_color = most_common_color
                    self.last_change_time = time.time()
                    return change_info
        return None

class ProfessionalSmartBlackBox:
    """IoU ê¸°ë°˜ ì°¨ëŸ‰ ì¶”ì  ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config_path="blackbox_config.json"):
        self.setup_logging()
        self.config_path = config_path
        self.config = self.load_config()
        self.camera_manager = CameraManager(self.logger)
        self.net = None
        self.classes = []
        self.output_layers = []
        self.running = False
        self.current_frame = None
        self.last_valid_frame = None
        self.video_writer = None
        self.frame_buffer = deque(maxlen=300)
        self.vehicle_tracker = SimpleIOUTracker(
            max_lost=5, 
            iou_threshold=0.3, 
            movement_threshold=2
        )
        self.traffic_light_detector = TrafficLightColorDetector()
        self.lane_detector = LaneDetector()
        self.event_log_file = "./text/event_log.txt"
        self.detection_stats = {
            'total_detections': 0,
            'vehicles': 0,
            'persons': 0,
            'traffic_lights': 0,
            'traffic_light_changes': 0,
            'vehicle_departures': 0
        }
        self.system_stats = {
            'cpu': 0,
            'memory': 0,
            'disk': 0,
            'temperature': 'N/A'
        }
        self.last_cleanup = 0
        self.last_detection_time = 0
        self.last_detections = []
        self.last_incident_report = 0
        self.frame_read_failures = 0
        self.max_frame_read_failures = 10
        self.logger.info("âœ… IoU ê¸°ë°˜ ì°¨ëŸ‰ ì¶”ì  ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        self.tts_system = TTSNavigationSystem()
        self.TTS_ALLOWED = ["ì‹ í˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤", "ì•ì˜ ì°¨ëŸ‰ì´ ì¶œë°œí•˜ì˜€ìŠµë‹ˆë‹¤"]

        # --- ë¡œê·¸ ë¹„ë™ê¸°í™” ë° TTS ì¤‘ë³µ ë°©ì§€ ìµœì í™” ---
        self.log_queue = queue.Queue(maxsize=1000)
        self.log_buffer = []
        self.log_thread = threading.Thread(target=self._log_worker, daemon=True)
        self.log_thread.start()
        self.tts_message_hashes = set()
        # -------------------------------------------
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('blackbox.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_config(self):
        default_config = {
            "camera": {
                "device_id": "auto",
                "width": 1280,
                "height": 720,
                "fps": 30,
                "buffer_size": 1
            },
            "recording": {
                "output_dir": "recordings",
                "segment_duration": 600,
                "codec": "mp4v",
                "quality": 0.8
            },
            "storage": {
                "recordings_dir": "recordings",
            },
            "video": {
                "fps": 30
            },
            "model": {
                "weights_path": "yolov4.weights",
                "config_path": "yolov4.cfg",
                "classes_path": "coco.names",
                "input_size": 416
            },
            "detection": {
                "confidence_threshold": 0.5,
                "nms_threshold": 0.4,
                "traffic_light_confidence_threshold": 0.3,
                "detection_interval": 1,
                "traffic_light_only_interval": 1
            },
            "traffic_light": {
                "enable_detection": True,
                "stability_frames": 2,
                "log_changes": True
            },
            "tracking": {
                "iou_threshold": 0.3,
                "max_lost_frames": 5,
                "movement_threshold": 2  # 5 â†’ 2ë¡œ ë³€ê²½
            },
            "system": {
                "cleanup_interval": 3600,
                "max_storage_gb": 50,
                "cpu_threshold": 80,
                "memory_threshold": 80
            }
        }
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                    elif isinstance(value, dict):
                        for subkey, subvalue in value.items():
                            if subkey not in config[key]:
                                config[key][subkey] = subvalue
                return config
            else:
                with open(self.config_path, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, indent=2, ensure_ascii=False)
                return default_config
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return default_config
    
    def init_camera(self):
        return self.camera_manager.init_camera_auto()
    
    def init_ai_model(self):
        try:
            weights_path = self.config['model']['weights_path']
            config_path = self.config['model']['config_path']
            classes_path = self.config['model']['classes_path']
            
            if not all(os.path.exists(p) for p in [weights_path, config_path, classes_path]):
                self.logger.warning("AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë…¹í™” ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return False
            
            self.net = cv2.dnn.readNet(weights_path, config_path)
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            
            with open(classes_path, 'r') as f:
                self.classes = [line.strip() for line in f.readlines()]
            
            layer_names = self.net.getLayerNames()
            self.output_layers = [layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]
            
            self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.classes)}ê°œ í´ë˜ìŠ¤")
            return True
        except Exception as e:
            self.logger.error(f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def create_video_writer(self, frame_shape):
        try:
            output_dir = self.config['storage']['recordings_dir']
            os.makedirs(output_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"REC_{timestamp}.mp4"
            filepath = os.path.join(output_dir, filename)
            height, width = frame_shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            fps = self.config['video']['fps']
            if width % 2 != 0:
                width -= 1
            if height % 2 != 0:
                height -= 1
            writer = cv2.VideoWriter(filepath, fourcc, fps, (width, height))
            if writer.isOpened():
                self.logger.info(f"âœ… ë…¹í™” ì‹œì‘: {filename} ({width}x{height})")
                return writer, filepath
            else:
                self.logger.error(f"âŒ ë¹„ë””ì˜¤ ë¼ì´í„° ìƒì„± ì‹¤íŒ¨: {filepath}")
                return None, None
        except Exception as e:
            self.logger.error(f"ë¹„ë””ì˜¤ ë¼ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return None, None
    
    def filter_vehicles_by_lane(self, vehicle_detections, frame_width, frame_height, frame):
        lanes = self.lane_detector.detect_lanes(frame)
        if lanes is None:
            return vehicle_detections
        center_points = self.lane_detector.calculate_lane_center_points(lanes, frame_height)
        if not center_points:
            return vehicle_detections
        filtered = []
        lane_width = 100
        min_area = (frame_width * frame_height) * 0.01
        for detection in vehicle_detections:
            x, y, w, h = detection['box']
            vehicle_center_x = x + w / 2
            vehicle_center_y = y + h / 2
            if vehicle_center_y < frame_height * 0.65:
                continue
            closest_point = min(center_points, key=lambda p: abs(p[1] - vehicle_center_y))
            lane_center_x = closest_point[0]
            if abs(vehicle_center_x - lane_center_x) < lane_width / 2 and w * h > min_area:
                detection['in_zone'] = True  # ì¡´ ë‚´ ì°¨ëŸ‰ ë§ˆí‚¹
                filtered.append(detection)
        return filtered
    
    def detect_objects_optimized(self, frame):
        height, width = frame.shape[:2]
        input_size = self.config['model'].get('input_size', 416)
        blob = cv2.dnn.blobFromImage(frame, 0.00392, (input_size, input_size), (0, 0, 0), True, crop=False)
        self.net.setInput(blob)
        outputs = self.net.forward(self.output_layers)
        boxes, confidences, class_ids = [], [], []
        confidence_threshold = self.config['detection']['confidence_threshold']
        traffic_light_confidence_threshold = self.config['detection']['traffic_light_confidence_threshold']
        for output in outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                class_name = self.classes[class_id]
                min_conf = traffic_light_confidence_threshold if class_name == 'traffic light' else confidence_threshold
                if confidence > min_conf:
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)
        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 
                                  traffic_light_confidence_threshold,
                                  self.config['detection']['nms_threshold'])
        detections, vehicle_detections, traffic_light_detections = [], [], []
        if len(indexes) > 0:
            for i in indexes.flatten():
                class_name = self.classes[class_ids[i]]
                detection = {
                    'class_name': class_name,
                    'confidence': confidences[i],
                    'box': boxes[i],
                    'timestamp': time.time()
                }
                detections.append(detection)
                if class_name == 'traffic light':
                    traffic_light_detections.append(detection)
                    self.detection_stats['traffic_lights'] += 1
                if class_name in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                    vehicle_detections.append(detection)
                    self.detection_stats['vehicles'] += 1
                self.detection_stats['total_detections'] += 1
                if class_name == 'person':
                    self.detection_stats['persons'] += 1
        filtered_vehicle_detections = self.filter_vehicles_by_lane(vehicle_detections, width, height, frame)
        tracked_vehicles = []
        if filtered_vehicle_detections:
            tracked_vehicles = self.vehicle_tracker.update(filtered_vehicle_detections)
            for track in tracked_vehicles:
                if track.get('departure_detected', False):
                    self.log_vehicle_departure(track)
                    track['departure_detected'] = False
        if traffic_light_detections and self.config['traffic_light']['enable_detection']:
            self.analyze_traffic_light_colors_fast(frame, traffic_light_detections)
        return detections, tracked_vehicles
    
    def analyze_traffic_light_colors_fast(self, frame, traffic_light_detections):
        for detection in traffic_light_detections:
            x, y, w, h = detection['box']
            roi = frame[max(0, y):min(frame.shape[0], y+h), max(0, x):min(frame.shape[1], x+w)]
            if roi.size > 100:
                detected_color = self.traffic_light_detector.detect_traffic_light_color(roi)
                color_change = self.traffic_light_detector.update_color_history(detected_color)
                if color_change:
                    self.handle_traffic_light_change_fast(color_change)

    def _log_worker(self):
        """ë¹„ë™ê¸° ë¡œê·¸ ê¸°ë¡ ì›Œì»¤"""
        while True:
            try:
                try:
                    msg = self.log_queue.get(timeout=0.2)
                    self.log_buffer.append(msg)
                except queue.Empty:
                    pass
                # ë²„í¼ê°€ 5ê°œ ì´ìƒì´ê±°ë‚˜, íê°€ ë¹„ì—ˆì„ ë•Œ ì“°ê¸°
                if len(self.log_buffer) >= 5 or (self.log_queue.empty() and self.log_buffer):
                    with open(self.event_log_file, "a", encoding="utf-8") as f:
                        f.write("\n".join(self.log_buffer) + "\n")
                    self.log_buffer.clear()
                time.sleep(0.01)
            except Exception as e:
                self.logger.error(f"ë¹„ë™ê¸° ë¡œê·¸ ì“°ê¸° ì‹¤íŒ¨: {e}")
    def handle_traffic_light_change_fast(self, change_info):
        log_message = "ì‹ í˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤"
        self.log_queue.put(log_message)
        self.logger.info(f"ğŸš¦ {log_message}")
        self._tts_announce_if_needed(log_message)
        self.detection_stats['traffic_light_changes'] += 1

    def log_vehicle_departure(self, track):
        if track.get('departure_detected', False):
            log_message = "ì•ì˜ ì°¨ëŸ‰ì´ ì¶œë°œí•˜ì˜€ìŠµë‹ˆë‹¤"
            self.log_queue.put(log_message)
            self.logger.info(f"ğŸš— ì°¨ëŸ‰ ì¶œë°œ: ID{track['id']}")
            self.detection_stats['vehicle_departures'] += 1
            self._tts_announce_if_needed(log_message)
    def _tts_announce_if_needed(self, msg):
        """í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€ TTS ì•ˆë‚´"""
        msg_hash = hashlib.md5(msg.encode()).hexdigest()
        if msg in self.TTS_ALLOWED and msg_hash not in self.tts_message_hashes:
            try:
                self.tts_system.play_situation_from_txt(msg, keyword=msg)
                self.tts_message_hashes.add(msg_hash)
                # ì˜¤ë˜ëœ í•´ì‹œëŠ” 1000ê°œ ì´ìƒì´ë©´ ì œê±°
                if len(self.tts_message_hashes) > 1000:
                    self.tts_message_hashes.pop()
            except Exception as e:
                self.logger.error(f"TTS ì•ˆë‚´ ì‹¤íŒ¨: {e}")
    
    def draw_lane_overlay(self, overlay, lanes, center_points, width, height):
        if lanes is None:
            return
        if lanes['left']:
            x1, y1, x2, y2 = lanes['left']
            cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 3)
        if lanes['right']:
            x1, y1, x2, y2 = lanes['right']
            cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 3)
        if center_points:
            for i in range(len(center_points) - 1):
                cv2.line(overlay, center_points[i], center_points[i+1], (255, 255, 0), 2)
            lane_width = 100
            for point in center_points:
                cx, cy = point
                cv2.rectangle(overlay, (cx - lane_width//2, cy - 2), (cx + lane_width//2, cy + 2), (0, 255, 255), -1)
    
    def draw_optimized_overlay(self, frame, detections, tracked_vehicles):
        overlay = frame.copy()
        height, width = frame.shape[:2]
        lanes = self.lane_detector.last_valid_lanes
        center_points = self.lane_detector.calculate_lane_center_points(lanes, height) if lanes else None
        self.draw_lane_overlay(overlay, lanes, center_points, width, height)
        for detection in detections:
            x, y, w, h = detection['box']
            class_name = detection['class_name']
            confidence = detection['confidence']
            colors = {
                'traffic light': (255, 255, 255),
                'car': (0, 255, 0), 'truck': (255, 0, 0), 'bus': (0, 0, 255),
                'person': (255, 255, 0), 'bicycle': (255, 0, 255), 'motorbike': (0, 255, 255)
            }
            color = colors.get(class_name, (128, 128, 128))
            thickness = 3 if class_name == 'traffic light' else 2
            cv2.rectangle(overlay, (x, y), (x + w, y + h), color, thickness)
            if class_name == 'traffic light':
                current_color = self.traffic_light_detector.last_stable_color
                if current_color:
                    color_text = f"LIGHT: {current_color.upper()}"
                    cv2.putText(overlay, color_text, (x, y + h + 25), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            label = f"{class_name}: {confidence:.2f}"
            cv2.putText(overlay, label, (x, y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        for track in tracked_vehicles:
            x, y, w, h = track['bbox']
            track_id = track['id']
            class_name = track['class_name']
            track_color = (0, 0, 255) if track['is_moving'] else (0, 255, 0)
            cv2.rectangle(overlay, (x, y), (x + w, y + h), track_color, 3)
            status = "MOVING" if track['is_moving'] else "STATIC"
            track_label = f"ID:{track_id} {class_name} {status}"
            cv2.putText(overlay, track_label, (x, y - 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, track_color, 2)
        panel_height = 140
        cv2.rectangle(overlay, (0, 0), (overlay.shape[1], panel_height), (0, 0, 0), -1)
        timestamp = datetime.now().strftime("%H:%M:%S")
        cv2.putText(overlay, f"TIME: {timestamp}", (10, 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(overlay, "LANE-BASED FRONT VEHICLE TRACKING", (10, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        active_tracks = len(tracked_vehicles)
        moving_tracks = len([t for t in tracked_vehicles if t['is_moving']])
        stats_text = f"Tracks:{active_tracks} Moving:{moving_tracks} TL:{self.detection_stats['traffic_lights']} TC:{self.detection_stats['traffic_light_changes']}"
        cv2.putText(overlay, stats_text, (10, 75), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        departure_text = f"Departures: {self.detection_stats['vehicle_departures']}"
        cv2.putText(overlay, departure_text, (10, 100), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        if self.traffic_light_detector.last_stable_color:
            light_status = f"SIGNAL: {self.traffic_light_detector.last_stable_color.upper()}"
            cv2.putText(overlay, light_status, (overlay.shape[1] - 250, 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        return overlay
    
    def monitor_system_resources(self):
        try:
            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent
            self.system_stats = {
                'cpu': cpu_percent,
                'memory': memory_percent,
                'disk': 0,
                'temperature': 'N/A'
            }
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
    
    def cleanup_old_files(self):
        try:
            output_dir = self.config['storage']['recordings_dir']
            if not os.path.exists(output_dir):
                return
            max_size_bytes = self.config['system']['max_storage_gb'] * 1024 * 1024 * 1024
            files = []
            total_size = 0
            for filename in os.listdir(output_dir):
                filepath = os.path.join(output_dir, filename)
                if os.path.isfile(filepath):
                    size = os.path.getsize(filepath)
                    mtime = os.path.getmtime(filepath)
                    files.append((filepath, size, mtime))
                    total_size += size
            if total_size > max_size_bytes:
                files.sort(key=lambda x: x[2])
                for filepath, size, mtime in files:
                    try:
                        os.remove(filepath)
                        total_size -= size
                        self.logger.info(f"ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {os.path.basename(filepath)}")
                        if total_size <= max_size_bytes * 0.8:
                            break
                    except Exception as e:
                        self.logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {filepath}: {e}")
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def signal_handler(self, signum, frame):
        self.running = False
        self.cleanup_resources()
        sys.exit(0)
    
    def cleanup_resources(self):
        # ë¡œê·¸ ë²„í¼ ë§ˆì§€ë§‰ìœ¼ë¡œ ë¹„ìš°ê¸°
        if self.log_buffer:
            try:
                with open(self.event_log_file, "a", encoding="utf-8") as f:
                    f.write("\n".join(self.log_buffer) + "\n")
                self.log_buffer.clear()
            except Exception as e:
                self.logger.error(f"ë¡œê·¸ í”ŒëŸ¬ì‹œ ì‹¤íŒ¨: {e}")
        # ... ê¸°ì¡´ ì •ë¦¬ ì½”ë“œ ...
        try:
            if self.video_writer:
                self.video_writer.release()
            if self.camera_manager.camera:
                self.camera_manager.camera.release()
            cv2.destroyAllWindows()
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def run_optimized(self):
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        if not self.init_camera():
            return False
        if not self.init_ai_model():
            self.logger.warning("AI ëª¨ë¸ ì—†ì´ ê¸°ë³¸ ë…¹í™” ëª¨ë“œë¡œ ì‹¤í–‰")
        self.running = True
        frame_count = 0
        segment_start_time = time.time()
        last_fps_time = time.time()
        fps_counter = 0
        while self.running:
            try:
                ret, frame = self.camera_manager.camera.read()
                if not ret or frame is None:
                    self.frame_read_failures += 1
                    if self.frame_read_failures > self.max_frame_read_failures:
                        self.logger.error("ì—°ì† í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨, ì¢…ë£Œ")
                        break
                    self.logger.warning("í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    time.sleep(0.01)
                    continue
                self.frame_read_failures = 0
                self.current_frame = frame.copy()
                self.frame_buffer.append(frame.copy())
                if self.video_writer is None or (time.time() - segment_start_time) > self.config['recording']['segment_duration']:
                    if self.video_writer:
                        self.video_writer.release()
                    self.video_writer, video_path = self.create_video_writer(frame.shape)
                    if self.video_writer is None:
                        break
                    segment_start_time = time.time()
                detections = []
                tracked_vehicles = []
                detection_interval = self.config['detection']['detection_interval']
                traffic_light_interval = self.config['detection'].get('traffic_light_only_interval', 1)
                if self.net is not None:
                    if frame_count % traffic_light_interval == 0:
                        detections, tracked_vehicles = self.detect_objects_optimized(frame)
                    elif frame_count % detection_interval == 0:
                        detections, tracked_vehicles = self.detect_objects_optimized(frame)
                overlay_frame = self.draw_optimized_overlay(frame.copy(), detections, tracked_vehicles)
                self.current_frame = overlay_frame.copy()
                if self.video_writer and self.video_writer.isOpened():
                    try:
                        if overlay_frame.shape[1] % 2 != 0:
                            overlay_frame = overlay_frame[:, :-1]
                        if overlay_frame.shape[0] % 2 != 0:
                            overlay_frame = overlay_frame[:-1, :]
                        self.video_writer.write(overlay_frame)
                    except Exception as e:
                        self.logger.error(f"í”„ë ˆì„ ì €ì¥ ì‹¤íŒ¨: {e}")
                fps_counter += 1
                if fps_counter % 30 == 0:
                    current_time = time.time()
                    fps = 30 / (current_time - last_fps_time)
                    last_fps_time = current_time
                    active_tracks = len(tracked_vehicles)
                    moving_vehicles = len([t for t in tracked_vehicles if t['is_moving']])
                    self.logger.info(f"ğŸ“Š FPS: {fps:.1f}, ì¶”ì : {active_tracks}, ì´ë™ì¤‘: {moving_vehicles}, ì‹ í˜¸ë“±: {self.detection_stats['traffic_lights']}, ë³€í™”: {self.detection_stats['traffic_light_changes']}, ì¶œë°œ: {self.detection_stats['vehicle_departures']}")
                if frame_count % 100 == 0:
                    self.monitor_system_resources()
                current_time = time.time()
                if current_time - self.last_cleanup > self.config['system']['cleanup_interval']:
                    self.cleanup_old_files()
                    self.last_cleanup = current_time
                frame_count += 1
            except KeyboardInterrupt:
                break
            except Exception as e:
                self.logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        self.cleanup_resources()
        return True

def main():
    parser = argparse.ArgumentParser(description='IoU ê¸°ë°˜ ì°¨ëŸ‰ ì¶”ì  ë¸”ë™ë°•ìŠ¤ (ì°¨ì„  ê¸°ë°˜ ë™ì  ì•ì°¨ ê°ì§€ + TTS ìµœì í™”)')
    parser.add_argument('--config', default='blackbox_config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    args = parser.parse_args()
    try:
        blackbox = ProfessionalSmartBlackBox(args.config)
        success = blackbox.run_optimized()
        if success:
            pass
        else:
            return 1
    except KeyboardInterrupt:
        pass
    except Exception as e:
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1
    finally:
        pass
    return 0

if __name__ == "__main__":
    sys.exit(main())