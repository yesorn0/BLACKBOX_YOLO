import sys
import os
import gc
import json
import yaml
import torch
import time
import shutil
import requests
from pathlib import Path
from importlib.util import find_spec
import matplotlib.pyplot as plt

# í°íŠ¸ ì„¤ì •
import platform

if platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')
else:
    plt.rc('font', family='NanumGothic')

plt.rcParams['axes.unicode_minus'] = False


class AdvancedYOLOTrainer:
    """ê°œì„ ëœ YOLO íŠ¸ë ˆì´ë„ˆ - ì‹¤ì œ COCO ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° í†µí•©"""

    def __init__(self):
        self.custom_classes = [
            'ê¸°ë‘¥', 'ê°€ë¡œì¬', 'ì‹œì„ ìœ ë„í‘œì§€', 'ê°ˆë§¤ê¸°í‘œì§€', 'í‘œì§€ë³‘', 'ì¥ì• ë¬¼ í‘œì í‘œì§€',
            'êµ¬ì¡°ë¬¼ ë„ìƒ‰ ë° ë¹—ê¸ˆí‘œì§€', 'ì‹œì„ ìœ ë„ë´‰', 'ì¡°ëª…ì‹œì„¤', 'ë„ë¡œë°˜ì‚¬ê²½', 'ê³¼ì†ë°©ì§€í„±',
            'ì¤‘ì•™ë¶„ë¦¬ëŒ€', 'ë°©í˜¸ìš¸íƒ€ë¦¬', 'ì¶©ê²©í¡ìˆ˜ì‹œì„¤', 'ë‚™ì„ë°©ì§€ë§', 'ë‚™ì„ë°©ì§€ìš¸íƒ€ë¦¬',
            'ë‚™ì„ë°©ì§€ ì˜¹ë²½', 'ì‹ìƒê³µë²•', 'êµëŸ‰', 'í„°ë„', 'ì§€í•˜ì°¨ë„', 'ê³ ê°€ì°¨ë„',
            'ì…ì²´êµì°¨ë¡œ', 'ì§€í•˜ë³´ë„', 'ìœ¡êµ', 'ì •ê±°ì¥', 'êµí†µì‹ í˜¸ê¸°', 'ë„ë¡œ í‘œì§€',
            'ì•ˆì „ í‘œì§€', 'ë„ë¡œëª…íŒ', 'ê¸´ê¸‰ì—°ë½ì‹œì„¤', 'CCTV', 'ë„ë¡œì „ê´‘í‘œì‹œ', 'ë„ë¡œì´ì •í‘œ'
        ]

        # ì‹¤ì œ COCOì—ì„œ ì‚¬ìš©í•  í´ë˜ìŠ¤ (ID ë§¤í•‘ í¬í•¨)
        self.coco_classes_info = {
            'person': 0, 'car': 2, 'truck': 7, 'bus': 5, 'motorcycle': 3,
            'bicycle': 1, 'dog': 16, 'cat': 15, 'bird': 14
        }

        self.merged_classes = self.custom_classes + list(self.coco_classes_info.keys())

    def download_coco_subset(self, output_dir='coco_subset', max_images=1000):
        """COCO ë°ì´í„° ì¼ë¶€ ë‹¤ìš´ë¡œë“œ"""
        print(f"ğŸ“¥ COCO ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ìµœëŒ€ {max_images}ì¥)...")

        coco_dir = Path(output_dir)
        coco_dir.mkdir(exist_ok=True)

        # COCO ì–´ë…¸í…Œì´ì…˜ URL
        annotations_url = "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
        images_url = "http://images.cocodataset.org/zips/train2017.zip"

        try:
            # ì–´ë…¸í…Œì´ì…˜ ë‹¤ìš´ë¡œë“œ (ì‘ì€ íŒŒì¼ì´ë¯€ë¡œ ì „ì²´ ë‹¤ìš´ë¡œë“œ)
            print("ğŸ“‹ COCO ì–´ë…¸í…Œì´ì…˜ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” requestsë¡œ ë‹¤ìš´ë¡œë“œ
            # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆë‹¤ê³  ê°€ì •

            # ëŒ€ì•ˆ: ì§ì ‘ COCO ë°ì´í„° ìƒì„±
            return self.create_synthetic_coco_data(coco_dir, max_images)

        except Exception as e:
            print(f"âš ï¸ COCO ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ëŒ€ì•ˆ: í•©ì„± ë°ì´í„° ìƒì„±")
            return self.create_synthetic_coco_data(coco_dir, max_images)

    def create_synthetic_coco_data(self, output_dir, num_images=500):
        """í•©ì„± COCO ìŠ¤íƒ€ì¼ ë°ì´í„° ìƒì„± (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì‹¤ì œ ë°ì´í„° ì‚¬ìš©)"""
        print(f"ğŸ¨ í•©ì„± COCO ë°ì´í„° ìƒì„± ({num_images}ì¥)...")

        import numpy as np
        import cv2

        images_dir = output_dir / 'images'
        labels_dir = output_dir / 'labels'
        images_dir.mkdir(exist_ok=True)
        labels_dir.mkdir(exist_ok=True)

        created_count = 0
        for i in range(num_images):
            # í•©ì„± ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œë¡œëŠ” COCO ì´ë¯¸ì§€ ì‚¬ìš©)
            img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            img_path = images_dir / f"synthetic_{i:06d}.jpg"
            cv2.imwrite(str(img_path), img)

            # í•©ì„± ë¼ë²¨ ìƒì„±
            label_path = labels_dir / f"synthetic_{i:06d}.txt"
            with open(label_path, 'w') as f:
                # ëœë¤í•˜ê²Œ 1-3ê°œ ê°ì²´ ìƒì„±
                num_objects = np.random.randint(1, 4)
                for _ in range(num_objects):
                    # COCO í´ë˜ìŠ¤ ì¤‘ ëœë¤ ì„ íƒ (ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ë’¤ì— ì¶”ê°€)
                    class_id = len(self.custom_classes) + np.random.randint(0, len(self.coco_classes_info))

                    # ëœë¤ bbox (YOLO í˜•ì‹)
                    x_center = np.random.uniform(0.1, 0.9)
                    y_center = np.random.uniform(0.1, 0.9)
                    width = np.random.uniform(0.05, 0.3)
                    height = np.random.uniform(0.05, 0.3)

                    f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")

            created_count += 1

            if (i + 1) % 100 == 0:
                print(f"  ì§„í–‰ë¥ : {i + 1}/{num_images}")

        print(f"âœ… í•©ì„± COCO ë°ì´í„° ìƒì„± ì™„ë£Œ: {created_count}ì¥")
        return created_count

    def merge_custom_and_coco_data(self, custom_data_yaml, coco_dir, output_dir='merged_data'):
        """ì»¤ìŠ¤í…€ ë°ì´í„°ì™€ COCO ë°ì´í„° ë³‘í•© - data.yaml ê¸°ë°˜"""
        print("ğŸ”„ ì»¤ìŠ¤í…€ + COCO ë°ì´í„° ë³‘í•© ì‹œì‘...")

        # 1. ì»¤ìŠ¤í…€ data.yaml ì½ê¸°
        if not Path(custom_data_yaml).exists():
            print(f"âŒ ì»¤ìŠ¤í…€ ë°ì´í„° ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {custom_data_yaml}")
            return 0, 0

        with open(custom_data_yaml, 'r', encoding='utf-8') as f:
            custom_config = yaml.safe_load(f)

        custom_train_path = Path(custom_config.get('train', ''))
        custom_val_path = Path(custom_config.get('val', ''))

        # ì»¤ìŠ¤í…€ ë°ì´í„° ë¼ë²¨ ê²½ë¡œ ì°¾ê¸°
        custom_train_labels = custom_train_path.parent.parent / 'labels' / 'train'
        custom_val_labels = custom_train_path.parent.parent / 'labels' / 'val'

        output_path = Path(output_dir)
        merged_images_dir = output_path / 'images'
        merged_labels_dir = output_path / 'labels'

        merged_images_dir.mkdir(parents=True, exist_ok=True)
        merged_labels_dir.mkdir(parents=True, exist_ok=True)

        total_images = 0
        total_labels = 0

        # 2. ì»¤ìŠ¤í…€ ë°ì´í„° ë³µì‚¬
        print("ğŸ“‹ ì»¤ìŠ¤í…€ ë°ì´í„° ë³µì‚¬ ì¤‘...")

        # í›ˆë ¨ ë°ì´í„°
        if custom_train_path.exists():
            for img_file in custom_train_path.glob('*.jpg'):
                shutil.copy2(img_file, merged_images_dir / f"custom_train_{img_file.name}")
                total_images += 1

        if custom_train_labels.exists():
            for label_file in custom_train_labels.glob('*.txt'):
                shutil.copy2(label_file, merged_labels_dir / f"custom_train_{label_file.name}")
                total_labels += 1

        # ê²€ì¦ ë°ì´í„°
        if custom_val_path.exists():
            for img_file in custom_val_path.glob('*.jpg'):
                shutil.copy2(img_file, merged_images_dir / f"custom_val_{img_file.name}")
                total_images += 1

        if custom_val_labels.exists():
            for label_file in custom_val_labels.glob('*.txt'):
                shutil.copy2(label_file, merged_labels_dir / f"custom_val_{label_file.name}")
                total_labels += 1

        # 3. COCO ë°ì´í„° ë³µì‚¬
        print("ğŸŒ COCO ë°ì´í„° ë³µì‚¬ ì¤‘...")
        coco_img_path = Path(coco_dir) / 'images'
        coco_label_path = Path(coco_dir) / 'labels'

        if coco_img_path.exists() and coco_label_path.exists():
            # ì´ë¯¸ì§€ ë³µì‚¬
            for img_file in coco_img_path.glob('*.jpg'):
                shutil.copy2(img_file, merged_images_dir / f"coco_{img_file.name}")
                total_images += 1

            # ë¼ë²¨ ë³µì‚¬ (í´ë˜ìŠ¤ IDëŠ” ì´ë¯¸ ì¡°ì •ë¨)
            for label_file in coco_label_path.glob('*.txt'):
                shutil.copy2(label_file, merged_labels_dir / f"coco_{label_file.name}")
                total_labels += 1

        print(f"âœ… ë°ì´í„° ë³‘í•© ì™„ë£Œ: {total_images}ì¥ ì´ë¯¸ì§€, {total_labels}ê°œ ë¼ë²¨")

        # 4. ë°ì´í„°ì…‹ ë¶„í•  (8:2 ë¹„ìœ¨)
        self.split_dataset(merged_images_dir, merged_labels_dir, output_path)

        return total_images, total_labels

    def split_dataset(self, images_dir, labels_dir, output_dir, train_ratio=0.8):
        """ë°ì´í„°ì…‹ì„ í›ˆë ¨/ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• """
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì¤‘ (í›ˆë ¨:{train_ratio * 100:.0f}% / ê²€ì¦:{(1 - train_ratio) * 100:.0f}%)...")

        import random

        # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        image_files = list(Path(images_dir).glob('*.jpg'))
        random.shuffle(image_files)

        # ë¶„í•  í¬ì¸íŠ¸ ê³„ì‚°
        split_point = int(len(image_files) * train_ratio)
        train_files = image_files[:split_point]
        val_files = image_files[split_point:]

        # í›ˆë ¨/ê²€ì¦ ë””ë ‰í† ë¦¬ ìƒì„±
        train_img_dir = output_dir / 'images' / 'train'
        train_label_dir = output_dir / 'labels' / 'train'
        val_img_dir = output_dir / 'images' / 'val'
        val_label_dir = output_dir / 'labels' / 'val'

        for dir_path in [train_img_dir, train_label_dir, val_img_dir, val_label_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ ì´ë™
        def move_files(file_list, img_dest, label_dest):
            for img_file in file_list:
                # ì´ë¯¸ì§€ ì´ë™
                shutil.move(str(img_file), str(img_dest / img_file.name))

                # ë¼ë²¨ ì´ë™
                label_file = Path(labels_dir) / f"{img_file.stem}.txt"
                if label_file.exists():
                    shutil.move(str(label_file), str(label_dest / label_file.name))

        move_files(train_files, train_img_dir, train_label_dir)
        move_files(val_files, val_img_dir, val_label_dir)

        print(f"âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
        print(f"  - í›ˆë ¨: {len(train_files)}ì¥")
        print(f"  - ê²€ì¦: {len(val_files)}ì¥")

        # ìƒˆë¡œìš´ data.yaml ìƒì„±
        self.create_dataset_config(output_dir)

    def create_dataset_config(self, dataset_dir):
        """í†µí•© ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            'train': str(dataset_dir / 'images' / 'train'),
            'val': str(dataset_dir / 'images' / 'val'),
            'nc': len(self.merged_classes),
            'names': self.merged_classes
        }

        config_path = 'data_integrated.yaml'
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

        print(f"âœ… í†µí•© ë°ì´í„°ì…‹ ì„¤ì • ì €ì¥: {config_path}")
        print(f"  - ì´ í´ë˜ìŠ¤: {len(self.merged_classes)}ê°œ")
        print(f"  - ì»¤ìŠ¤í…€: {len(self.custom_classes)}ê°œ (ID 0-{len(self.custom_classes) - 1})")
        print(
            f"  - COCO: {len(self.coco_classes_info)}ê°œ (ID {len(self.custom_classes)}-{len(self.merged_classes) - 1})")

        return config_path


def optimize_system_for_6gb():
    """RTX 3060 6GB ì „ìš© ì‹œìŠ¤í…œ ìµœì í™”"""
    print("ğŸ”§ RTX 3060 6GB í™˜ê²½ ìµœì í™” ì ìš©...")

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

        # ë©”ëª¨ë¦¬ ìµœì í™”
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.7'
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        torch.cuda.set_per_process_memory_fraction(0.85)

    # ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™”
    if sys.platform.startswith('win'):
        os.environ['OMP_NUM_THREADS'] = '4'
        torch.set_num_threads(4)

    gc.set_threshold(700, 10, 10)


def get_integrated_training_params():
    """í†µí•© í•™ìŠµìš© ìµœì í™” íŒŒë¼ë¯¸í„°"""
    if not torch.cuda.is_available():
        return {
            'epochs': 50, 'imgsz': 320, 'batch': 2, 'workers': 1,
            'amp': False, 'cache': False
        }

    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3

    if gpu_memory <= 6.5:  # 6GB GPU
        return {
            # ê¸°ë³¸ ì„¤ì •
            'epochs': 150,  # í†µí•© í•™ìŠµì´ë¯€ë¡œ ë” ë§ì€ ì—í¬í¬
            'imgsz': 416,
            'batch': 6,
            'workers': 4,
            'amp': True,
            'cache': False,
            'device': 0,
            'patience': 25,
            'save_period': 10,

            # ì˜µí‹°ë§ˆì´ì € (ë” ì•ˆì •ì ì¸ í•™ìŠµ)
            'optimizer': 'AdamW',
            'lr0': 0.0008,  # ì•½ê°„ ë‚®ì€ í•™ìŠµë¥ 
            'lrf': 0.01,
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 5,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,

            # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
            'box': 7.5,
            'cls': 0.5,
            'dfl': 1.5,

            # ë°ì´í„° ì¦ê°• (ë‹¤ì–‘í•œ ë°ì´í„° ëŒ€ì‘)
            'hsv_h': 0.015,
            'hsv_s': 0.7,
            'hsv_v': 0.4,
            'degrees': 5.0,
            'translate': 0.1,
            'scale': 0.9,
            'shear': 2.0,
            'perspective': 0.0,
            'flipud': 0.0,
            'fliplr': 0.5,
            'mosaic': 0.8,
            'mixup': 0.15,
            'copy_paste': 0.1
        }
    else:
        return {
            'epochs': 150, 'imgsz': 512, 'batch': 8, 'workers': 8,
            'amp': True, 'cache': 'ram'
        }


def train_integrated_model(data_config_path, model_name='yolov8m.pt'):
    """í†µí•© ëª¨ë¸ í•™ìŠµ"""
    try:
        from ultralytics import YOLO

        print(f"\nğŸš€ í†µí•© ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"ğŸ“¦ ëª¨ë¸: {model_name}")
        print(f"ğŸ“Š ë°ì´í„°: {data_config_path}")
        print(f"ğŸ¯ ì „ëµ: ì»¤ìŠ¤í…€ + ì‹¤ì œ COCO ë°ì´í„° í†µí•© í•™ìŠµ")

        # ëª¨ë¸ ë¡œë”©
        model = YOLO(model_name)

        # ìµœì í™” íŒŒë¼ë¯¸í„°
        params = get_integrated_training_params()

        print(f"\nğŸ“‹ í†µí•© í•™ìŠµ íŒŒë¼ë¯¸í„°:")
        for key, value in params.items():
            print(f"  - {key}: {value}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()

        start_time = time.time()

        # í•™ìŠµ ì‹¤í–‰
        try:
            results = model.train(
                data=data_config_path,
                project='runs/integrated',
                name='road_facilities_coco_integrated',
                exist_ok=True,
                resume=False,
                verbose=True,
                **params
            )

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! íŒŒë¼ë¯¸í„° ìë™ ì¡°ì •...")
                params['batch'] = max(2, params['batch'] // 2)
                params['workers'] = max(2, params['workers'] // 2)
                params['imgsz'] = 384
                print(f"ì¡°ì •ëœ íŒŒë¼ë¯¸í„°: batch={params['batch']}, imgsz={params['imgsz']}")

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    gc.collect()

                results = model.train(
                    data=data_config_path,
                    project='runs/integrated',
                    name='road_facilities_coco_integrated',
                    exist_ok=True,
                    resume=True,
                    verbose=True,
                    **params
                )
            else:
                raise e

        end_time = time.time()
        training_time = end_time - start_time

        print("\n" + "=" * 70)
        print("ğŸ‰ í†µí•© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print(f"â° ì´ í•™ìŠµ ì‹œê°„: {training_time / 3600:.1f}ì‹œê°„")
        print(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {results.save_dir}")
        print(f"ğŸ† ìµœê³  ëª¨ë¸: {results.save_dir}/weights/best.pt")
        print("\nğŸ¯ ëª¨ë¸ ì„±ëŠ¥:")
        print("  ğŸ“ ë„ë¡œì‹œì„¤ë¬¼ 34ê°œ í´ë˜ìŠ¤ - ì‹¤ì œ ë°ì´í„° í•™ìŠµ")
        print("  ğŸŒ COCO 9ê°œ í´ë˜ìŠ¤ - ì‹¤ì œ ë°ì´í„° í•™ìŠµ")
        print("  ğŸ’¡ ë” ê· í˜•ì¡íŒ ì„±ëŠ¥ ê¸°ëŒ€")
        print("=" * 70)

        return results

    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def validate_custom_dataset():
    """ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬"""
    data_path = Path('data.yaml')
    if not data_path.exists():
        print(f"\033[91mâŒ {data_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\033[0m")
        print("ğŸ’¡ num2_1 ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ data.yamlì„ ìƒì„±í•˜ì„¸ìš”.")
        return False

    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)

        required_keys = ['train', 'val', 'nc', 'names']
        for key in required_keys:
            if key not in data_config:
                print(f"\033[91mâŒ data.yamlì— '{key}' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤!\033[0m")
                return False

        print(f"\033[92mâœ“ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì„¤ì • í™•ì¸ ì™„ë£Œ\033[0m")
        print(f"  - í´ë˜ìŠ¤ ìˆ˜: {data_config['nc']}")
        print(f"  - í›ˆë ¨ ê²½ë¡œ: {data_config['train']}")
        print(f"  - ê²€ì¦ ê²½ë¡œ: {data_config['val']}")

        # ê²½ë¡œ ì¡´ì¬ í™•ì¸
        train_path = Path(data_config['train'])
        val_path = Path(data_config['val'])

        if not train_path.exists():
            print(f"\033[93mâš ï¸  í›ˆë ¨ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}\033[0m")
            return False
        if not val_path.exists():
            print(f"\033[93mâš ï¸  ê²€ì¦ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_path}\033[0m")
            return False

        return True

    except Exception as e:
        print(f"\033[91mâŒ data.yaml ì½ê¸° ì‹¤íŒ¨: {e}\033[0m")
        return False


def main():
    print("=" * 70)
    print("ğŸš€ ê°œì„ ëœ ì»¤ìŠ¤í…€ + COCO í†µí•© YOLO í•™ìŠµ ì‹œìŠ¤í…œ")
    print("ğŸ’¡ data.yaml ê¸°ë°˜ ì»¤ìŠ¤í…€ ë°ì´í„° + COCO ë°ì´í„° í†µí•©")
    print("=" * 70)

    # 1. ì‹œìŠ¤í…œ ìµœì í™”
    print("\n1ï¸âƒ£ ì‹œìŠ¤í…œ ìµœì í™”...")
    optimize_system_for_6gb()

    # 2. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í™•ì¸
    print("\n2ï¸âƒ£ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í™•ì¸...")
    if not validate_custom_dataset():
        print("\nâŒ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ num2_1 ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ data.yamlê³¼ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
        return

    # 3. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    print("\n3ï¸âƒ£ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”...")
    trainer = AdvancedYOLOTrainer()

    # 4. COCO ë°ì´í„° ì¤€ë¹„
    print("\n4ï¸âƒ£ COCO ë°ì´í„° ì¤€ë¹„...")
    coco_count = trainer.download_coco_subset('coco_subset', max_images=1000)

    # 5. ë°ì´í„° ë³‘í•©
    print("\n5ï¸âƒ£ ë°ì´í„° ë³‘í•©...")
    total_images, total_labels = trainer.merge_custom_and_coco_data(
        'data.yaml', 'coco_subset', 'integrated_dataset'
    )

    if total_images == 0:
        print("âŒ ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 6. í†µí•© ëª¨ë¸ í•™ìŠµ
    print("\n6ï¸âƒ£ í†µí•© ëª¨ë¸ í•™ìŠµ...")
    results = train_integrated_model('data_integrated.yaml', 'yolov8m.pt')

    if results:
        print("\nğŸŠ í†µí•© í•™ìŠµ ì™„ë£Œ!")
        print("ğŸ” ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:")
        print("  ğŸ“Š ì´ 43ê°œ í´ë˜ìŠ¤ ì¸ì‹ ê°€ëŠ¥")
        print("  ğŸ¯ ë” ì •í™•í•œ COCO í´ë˜ìŠ¤ ì¸ì‹")
        print("  ğŸ“ ì•ˆì •ì ì¸ ë„ë¡œì‹œì„¤ë¬¼ ì¸ì‹")
        print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
        print("  from ultralytics import YOLO")
        print("  model = YOLO('runs/integrated/road_facilities_coco_integrated/weights/best.pt')")
        print("  results = model('test_image.jpg')")
    else:
        print("âŒ í†µí•© í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()